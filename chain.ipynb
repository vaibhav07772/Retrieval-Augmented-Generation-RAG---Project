{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171a9c5b",
   "metadata": {},
   "source": [
    "## Retriever And Chain With Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e78ef16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 0, 'page_label': '1'}, page_content='DATA SCIENCE \\nINTERVIEW PREPARATION \\n(30 Days of Interview \\nPreparation) \\n \\n# DAY 04'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 1, 'page_label': '2'}, page_content=\"Q1. What is upsampling and downsampling with examples? \\nThe classification data set with skewed class proportions is called an \\nimbalanced data set. Classes which make up a large proportion of the data \\nsets are called majority classes. Those make up smaller proportions are \\nminority classes. \\nDegree of imbalance Proportion of Minority Class \\n1>> Mild 20-40% of the data set \\n2>> Moderate 1-20% of the data set \\n3>> Extreme <1% of the data set \\nIf we have an imbalanced data set, first try training on the true distribution. \\nIf the model works well and generalises, you are done! If not, try the \\nfollowing up sampling and down sampling technique. \\n1. Up-sampling \\nUpsampling is the process of randomly duplicating observations from the \\nminority class to reinforce its signal. \\nFirst, we will import the resampling module from Scikit-Learn: \\nModule for resampling Python \\n1- From sklearn.utils import resample \\nNext, we will create a new Data Frame with an up-sampled minority class. \\nHere are the steps: \\n1- First, we will separate observations from each class into different Data \\nFrames. \\n2- Next, we will resample the minority class with replacement, setting the \\nnumber of samples to match that of the majority class.  \\n3- Finally, we'll combine the up-sampled minority class Data Frame with the \\noriginal majority class Data Frame. \\n2-Down-sampling \\nDownsampling involves randomly removing observations from the majority \\nclass to prevent its signal from dominating the learning algorithm.  \\nThe process is similar to that of sampling. Here are the steps: \\n1-First, we will separate observations from each class into different Data \\nFrames.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 2, 'page_label': '3'}, page_content='2-Next, we will resample the majority class without replacement, setting the \\nnumber of samples to match that of the minority class.  \\n3-Finally, we will combine the down-sampled majority class Data Frame \\nwith the original minority class Data Frame. \\nQ2. What is the statistical test for data validation with an example,  \\n        Chi-square, ANOVA test, Z statics, T statics, F statics,  \\n Hypothesis Testing? \\nBefore discussing the different statistical test, we need to get a clear \\nunderstanding of what a null hypothesis is. A null hypothesis proposes that \\nhas no significant difference exists in the set of a given observation . \\nNull:  Two samples mean are equal. Alternate: Two samples mean are not \\nequal. \\nFor rejecting the null hypothesis, a test is calculated. Then the test statistic \\nis compared with a critical value, and if found to be greater than the critical \\nvalue, the hypothesis will be rejected. \\nCritical Value:- \\nCritical values are the point beyond which we reject the null hypothesis. \\nCritical value tells us, what is the probability of N number of samples, \\nbelonging to the same distribution. Higher, the critical value which means \\nlower the probability of N number of samples belonging to the same \\ndistribution. \\nCritical values can be used to do hypothesis testing in the following way. \\n1. Calculate test statistic \\n2. Calculate critical values based on the significance level alpha \\n3. Compare test statistics with critical values. \\nIMP-If the test statistic is lower than the critical value, accept the hypothesis \\nor else reject the hypothesis. \\nChi-Square Test:- \\nA chi-square test is used if there is a relationship between two categorical \\nvariables.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 3, 'page_label': '4'}, page_content='Chi-Square test is used to determine whether there is a significant \\ndifference between the expected frequency and the observed frequency in \\none or more categories. Chi-square is also called the non-parametric test \\nas it will not use any parameter \\n \\n \\n \\n2-Anova test:- \\nANOVA, also called an analysis of variance, is used to compare multiples \\n(three or more) samples with a single test. \\nUseful when there are more than three populations. Anova compares the \\nvariance within and between the groups of the population . If the variation is \\nmuch larger than the within variation, the means of different samples will \\nnot be equal. If the between and within variations are approximately the \\nsame size, then there will be no significant difference between sample \\nmeans. Assumptions of ANOVA: 1-All populations involved follow a normal \\ndistribution. 2-All populations have the same variance (or standard \\ndeviation). 3-The samples are randomly selected and independent of one \\nanother. \\nANOVA uses the mean of the samples or the population to reject or \\nsupport the null hypothesis. Hence it is called parametric testing.  \\n3-Z Statics:- \\nIn a z-test, the samples are assumed to be normal distributed. A z score is \\ncalculated with population parameters as “population mean” and \\n“population standard deviation” and it is used to validate a hypothesis that \\nthe sample drawn belongs to the same population. \\nThe statistics used for this hypothesis testing is called z-statistic, the score \\nfor which is calculated as z = (x — μ) / (σ / √n), where x= sample mean μ = \\npopulation mean σ / √n = population standard deviation If the test statistic is \\nlower than the critical value, accept the hypothesis or else reject the \\nhypothesis \\n4- T Statics:- \\nA t-test used to compare the mean of the given samples. Like  z-test,  t-test \\nalso assumed a normal distribution of the samples. A t-test is used when \\nthe population parameters (mean and standard deviation) are unknown.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 4, 'page_label': '5'}, page_content='There are three versions of t-test \\n1. Independent samples t-test which compare means for two groups \\n2. Paired sample t-test which compares mean from the same group at \\ndifferent times \\n3. Sample t-test, which tests the mean of the single group against the \\nknown mean. The statistic for  hypothesis testing is called t-statistic, \\nthe score for which is calculated as t = (x1 — x2) / (σ / √n1 + σ / √n2), \\nwhere \\n              x1 =  It is mean of sample A, x2 = mean of sample B,  \\n              n1 = size of sample 1 n2 = size of    sample 2 \\n \\n \\n \\n5- F Statics:- \\nThe F-test is designed to test if the two population variances are equal. It \\ncompares the ratio of the two variances. Therefore, if the variances are \\nequal, then the ratio of the variances will be 1. \\nThe F-distribution is the ratio of two independent chi-square variables \\ndivided by their respective degrees of freedom. \\nF = s1^2 / s2^2 and where s1^2 > s2^2. \\nIf the null hypothesis is true, then the F test-statistic given above can be \\nsimplified. This ratio of sample variances will be tested statistic used. If the \\nnull hypothesis is false, then we will reject the null hypothesis that the ratio \\nwas equal to 1 and our assumption that they were equal. \\n \\nQ3. What is the Central limit theorem? \\nCentral Limit Theorem \\nDefinition: The theorem states that as the size of the sample increases, the \\ndistribution of the mean across multiple samples will approximate a \\nGaussian distribution (Normal). Generally, sample sizes equal to or greater \\nthan 30 are consider sufficient for the CLT to hold. It means that the \\ndistribution of the sample means is normally distributed. The average of the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 5, 'page_label': '6'}, page_content='sample means will be equal to the population mean. This is the key aspect \\nof the theorem. \\nAssumptions: \\n1. The data must follow the randomization condition. It must be sampled \\nrandomly \\n2. Samples should be independent of each other. One sample should \\nnot influence the other samples \\n3. Sample size should be no more than 10% of the population when \\nsampling is done without replacement \\n4. The sample size should be sufficiently large. The mean of the sample \\nmeans is denoted as: \\nµ X̄ = µ \\nWhere, \\nµ X̄ = Mean of the sample means µ= Population mean and, the standard \\ndeviation of the sample mean is denoted as: \\nσ X̄ = σ/sqrt(n) \\nWhere, \\nσ X̄ = Standard deviation of the sample mean σ = Population standard \\ndeviation n = sample size \\nA sufficiently large sample size can predict the characteristics of a \\npopulation accurately. For Example, we shall take a uniformly distributed \\ndata: \\nRandomly distributed data: Even for a randomly (Exponential) distributed \\ndata the plot of the means is normally distributed. \\nThe advantage of CLT is that we need not worry about the actual data \\nsince the means of it will always be normally distributed. With this, we can \\ncreate component intervals, perform T-tests and ANOVA tests from the \\ngiven samples. \\n \\nQ4. What is the correlation and coefficient? \\nWhat is the Correlation Coefficient? \\nThe correlation coefficient is a statistical measure that calculates the \\nstrength of the relationship between the relative movements of two'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 6, 'page_label': '7'}, page_content='variables. We use it to measure both the strength and direction of a linear \\nrelationship between two variables the values range between -1.0 and 1.0. \\nA calculated number greater than 1.0 or less than -1.0 means that there \\nwas an error in the correlation measurement. A correlation of -1.0 shows a \\nperfect negative correlation, while a correlation of 1.0 shows a perfect \\npositive correlation. \\n \\n \\n \\nCorrelation coefficient formulas are used to find how strong a relationship is \\nbetween data. The formulas return a value between -1 and 1, where: \\n1 indicates a strong positive relationship. -1 indicates a strong negative \\nrelationship. A result of zero indicates no relationship at all.  \\n \\nMeaning \\n1. A correlation coefficient of 1 means that for every positive increase in \\none variable, there is a positive increase in a fixed proportion in the \\nother. For example, shoe sizes go up in (almost) perfect correlation \\nwith foot length. \\n2. A correlation coefficient of -1 means that for every positive increase in \\none variable, there is a negative decrease of a fixed proportion in the \\nother. For example, the amount of gas in a tank decreases in  (almost) \\nperfect correlation with speed. \\n3. Zero means that for every increase, there isn’t a positive or negative \\nincrease. The two just aren’t related.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 7, 'page_label': '8'}, page_content='What is a Negative Correlation? \\nNegative correlation is a relationship between two variables in which one \\nvariable increases as the other decreases, and vice versa. In statistics, a \\nperfect negative correlation is represented by the value -1. Negative \\ncorrelation or inverse correlation is a relationship between two variables \\nwhereby they move in opposite directions. If variables X and Y have a \\nnegative correlation (or are negatively correlated), as X increases in value, \\nY will decrease; similarly, if X decreases in value, Y will increase.  \\nWhat Is Positive Correlation? \\nPositive correlation is a relationship between two variables in which both \\nvariables move in tandem—that is, in the same direction. A positive \\ncorrelation exists when one variable decreases as the other variable \\ndecreases or one variable increases while the other increases.  \\n \\n \\n \\nWe use the correlation coefficient to measure the strength and direction of \\nthe linear relationship between two numerical variables X and Y. The \\ncorrelation coefficient for a sample of data is denoted by r.  \\nPearson Correlation Coefficient \\nPearson is the most widely used correlation coefficient. Pearson correlation \\nmeasures the linear association between continuous variables. In other \\nwords, this coefficient quantifies the degree to which a relationship between \\ntwo variables can be described by a line. Formula developed by Karl \\nPearson over 120 years ago is still the most widely used today. The \\nformula for the correlation (r) is'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 8, 'page_label': '9'}, page_content='Where n is the number of pairs of data; \\nAre the sample means of all the x-values and all the y-values, respectively; \\nand sx and sy are the sample standard deviations of all the x- and y-values, \\nrespectively. \\n1. Find the mean of all the x-values and mean of all y-values. \\n2. Find the standard deviation of all the x-values (call it sx) and the \\nstandard deviation of all the y-values (call it sy). For example, to find \\nsx, you would use the following equation: \\n3. For each of the n pairs (x, y) in the data set, take \\n4. Add up the n results from Step 3. \\n5. Divide the sum by sx ∗ sy. \\n6. Divide the result by n – 1, where n is the number of (x, y) pairs. (It’s \\nthe same as multiplying by 1 over n – 1.) This gives you the \\ncorrelation, r. \\n \\nQ5: What is the difference between machine learning and deep  \\n       learning? \\nMachine Learning | deep learning \\nMachine Learning is a technique to learn from that data and then apply wha\\nt has been learnt to make an informed decision | The main difference betwe\\nen deep and machine learning is, machine learning models become better \\nprogressively but the model still needs some guidance. If a machine-\\nlearning model returns an inaccurate prediction then the programmer need\\ns to fix that problem explicitly but in the case of deep learning, the model do\\nes it by himself.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 9, 'page_label': '10'}, page_content='>Machine Learning can perform well with small size data also | Deep Learn\\ning does not perform as good with smaller datasets. \\n>Machine learning can work on some low-\\nend machines also  | Deep Learning involves many matrix multiplication op\\nerations which are better suited for GPUs \\n>Features need to be identified and extracted as per the domain before pu\\nshing them to the algorithm | Deep learning algorithms try to learn high-\\nlevel features from data. \\n>It is generally recommended to break the problem into smaller chunks, sol\\nve them and then combine the results | It generally focusses on solving the \\nproblem end to end \\n>Training time is comparatively less | Training time is comparatively more \\n>Results are more interpretable | Results Maybe more accurate but less int\\nerpretable \\n> No use of Neural networks | uses neural networks \\n> Solves comparatively less complex problems | Solves more complex prob\\nlems. \\nQ6: What is perceptron and how it is related to human neurons? \\nIf we focus on the structure of a biological neuron, it has dendrites, which \\nare used to receive inputs. These inputs are summed in the cell body and \\nusing the Axon it is passed on to the next biological neuron as shown \\nbelow.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 10, 'page_label': '11'}, page_content='Dendrite: Receives signals from other neurons \\nCell Body: Sums all the inputs \\nAxon: It is used to transmit signals to the other cells \\nSimilarly, a perceptron receives multiple inputs, applies various \\ntransformations and functions and provides an output. A Perceptron is a \\nlinear model used for binary classification. It models a neuron, which has a \\nset of inputs, each of which is given a specific weight. The neuron \\ncomputes some function on these weighted inputs and gives the output.  \\n \\n \\nQ7: Why deep learning is better than machine learning? \\nThough traditional ML algorithms solve a lot of our cases, they are not \\nuseful while working with high dimensional data that is where we have a \\nlarge number of inputs and outputs. For example, in the case of \\nhandwriting recognition, we have a large amount of input where we will \\nhave different types of inputs associated with different types of handwriting.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 11, 'page_label': '12'}, page_content='The second major challenge is to tell the computer what are the features it \\nshould look for that will play an important role in predicting the outcome a s \\nwell as to achieve better accuracy while doing so. \\nQ8: What kind of problem can be solved by using deep learning?  \\nDeep Learning is a branch of Machine Learning, which is used to solve \\nproblems in a way that mimics the human way of solving problems. \\nExamples: \\n\\uf0b7 Image recognition \\n\\uf0b7 Object Detection \\n\\uf0b7 Natural Language processing- Translation, Sentence formations, text \\nto speech, speech to text \\n\\uf0b7 understand the semantics of actions \\nQ9: List down all the activation function using mathematical    \\n       Expression and example. What is the activation function? \\nActivation functions are very important for an Artificial Neural Network to \\nlearn and make sense of something complicated and the Non-linear \\ncomplex functional mappings between the inputs and response variable. \\nThey introduce non-linear properties to our Network. Their main purposes \\nare to convert an input signal of a node in an A-NN to an output signal. \\nSo why do we need Non-Linearities?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 12, 'page_label': '13'}, page_content='Non-linear functions are those, which have a degree more than one, and \\nthey have a curvature when we plot a Non-Linear function. Now we need a \\nNeural Network Model to learn and represent almost anything and any \\narbitrary complex function, which maps inputs to outputs. Neural-Networks \\nare considered Universal Function Approximations. It means that they can \\ncompute and learn any function at all. \\nMost popular types of Activation functions - \\n\\uf0b7 Sigmoid or Logistic \\n\\uf0b7 Tanh — Hyperbolic tangent \\n\\uf0b7 ReLu -Rectified linear units \\nSigmoid Activation function: It is a activation function of form f(x) = 1 / 1 \\n+ exp(-x) . Its Range is between 0 and 1. It is an S-shaped curve. It is easy \\nto understand. \\n \\nHyperbolic Tangent function- Tanh : It’s mathematical formula is f(x) = 1 \\n— exp(-2x) / 1 + exp(-2x). Now it’s the output is zero centred because its \\nrange in between -1 to 1 i.e. -1 < output < 1 . Hence optimisation is easier \\nin this method; Hence in practice, it is always preferred over Sigmoid \\nfunction.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 13, 'page_label': '14'}, page_content='ReLu- Rectified Linear units: It has become more popular in the past \\ncouple of years. It was recently proved that it has six times improvement in \\nconvergence from Tanh function. It’s R(x) = max (0,x) i.e. if x < 0 , R(x) = 0 \\nand if x >= 0 , R(x) = x. Hence as seen that mathematical form of this \\nfunction, we can see that it is very simple and efficient. Many times in \\nMachine learning and computer science we notice that most simple and \\nconsistent techniques and methods are only preferred and  are the best. \\nHence, it avoids and rectifies the vanishing gradient problem. Almost all the \\ndeep learning Models use ReLu nowadays. \\n \\n \\nQ10: Detail explanation about gradient decent using example and  \\n       Mathematical expression?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 14, 'page_label': '15'}, page_content='Gradient descent is an optimisation algorithm used to minimize some \\nfunction by iteratively moving in the direction of steepest descent as \\ndefined by negative of the gradient. In machine learning, we used gradient \\ndescent to update the parameters of our model. Parameters refer t o \\ncoefficients in the Linear Regression and weights in neural networks.  \\n \\nThe size of these steps called the learning rate. With the high learning rate, \\nwe can cover more ground each step, but we risk overshooting the lower \\npoint since the slope of the hill is constantly changing. With a very lower \\nlearning rate, we can confidently move in the direction of the negativ e \\ngradient because we are recalculating it so frequently. The Lower learning \\nrate is more precise, but calculating the gradient is time -consuming, so it \\nwill take a very large time to get to the bottom. \\nMath \\nNow let’s run gradient descent using new cost function. There are two \\nparameters in cost function we can control: m (weight) and b (bias). Since \\nwe need to consider that the impact each one has on the final prediction, \\nwe need to use partial derivatives. We calculate the partial derivative of the \\ncost function concerning each parameter and store the results in a \\ngradient. \\nMath \\nGiven the cost function:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 15, 'page_label': '16'}, page_content='To solve for the gradient, we iterate by our data points using our new m \\nand b values and compute the partial derivatives. This new gradient te lls us \\nabout the slope of the cost function at our current position (current \\nparameter values) and the directions we should move to update our \\nparameters. The learning rate controls the size of our update. \\nQ11: What is backward propagation?  \\nBack-propagation is the essence of the neural net training and this \\nmethod of fine-tuning the weights of a neural net based on the errors rate \\nobtained in the previous epoch. Proper tuning of the weights allows us to \\nreduce error rates and to make the model reliable by increasing its \\ngeneralisation. \\nBackpropagation is a short form of \"backward propagation of errors.\" This \\nis the standard method of training artificial neural networks. This helps to \\ncalculate the gradient of a loss function with respects to all the weights in \\nthe network.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 16, 'page_label': '17'}, page_content='Most prominent advantages of Backpropagation are: \\n\\uf0b7 Backpropagation is the fast, simple and easy to program. \\n\\uf0b7 It has no parameters to tune apart from the numbers of input . \\n\\uf0b7 It is the flexible method as it does not require prior knowledge about \\nthe network \\n\\uf0b7 It is the standard method that generally works well. \\n\\uf0b7 It does not need any special mentions of the features of the function \\nto be learned. \\n \\n \\n \\nQ12: How we assign weights in deep learning? \\nWe already know that in a neural network, weights are usually initialised \\nrandomly and that kind of initialisation takes a fair/significant amount of \\nrepetitions to converge to the least loss and reach the ideal wei ght matrix. \\nThe problem is, that kind of initialisation is prone to vanishing or exploding \\ngradient problems.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 17, 'page_label': '18'}, page_content='General ways to make it initialise better weights: \\nReLu activation function in the deep nets. \\n1. Generate a random sample of weights from a Gaussian \\ndistribution having mean 0 and a standard deviation of 1.  \\n2. Multiply the sample with the square root of (2/ni). Where ni is the \\nnumber of input units for that layer. \\nb) Likewise, if you’re using Tanh activation function : \\n1. Generate a random sample of weights from a Gaussian distribution \\nhaving mean 0 and a standard deviation of 1. \\n2. Multiply the sample with the square root of (1/ni) where ni is several \\ninput units for that layer. \\n \\nQ13: What is optimiser is deep learning, and which one is the best? \\nDeep learning is an iterative process. With so many hyperparameters to \\ntune or methods to try, it is important to be able to train models fast, to \\nquickly complete the iterative cycle. This is the key to increase the speed \\nand efficiency of a machine learning team. \\nHence the importance of optimisation algorithms such as stochastic \\ngradient descent, min-batch gradient descent, gradient descent with \\nmomentum and the Adam optimiser. \\nAdam optimiser is the best one. \\nGiven an algorithm f(x), it helps in either minimisation or maximisation of \\nthe value of f(x). In this context of deep learning, we use optimisation \\nalgorithms to train the neural network by optimising the cost function J.  \\nThe cost function is defined as:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 18, 'page_label': '19'}, page_content='The value of the cost function J is the mean of the loss L between the \\npredicted value y’ and actual value y. The value y” is obtained during the \\nforward propagation step and makes use of the Weights W and biases b of \\nthe network. With the help of optimisation algorithms, we minimise the \\nvalue of Cost Function J  by updating the values of trainable \\nparameters W and b. \\nQ14: What is gradient descent, mini-batch gradient descent, batch  \\n         gradient decent, stochastic gradient decent and adam?  \\nGradient Descent \\nit is an iterative machine learning optimisation algorithm to reduce the cost \\nfunction, and help models to make accurate predictions. \\nGradient indicates the direction of increase. As we want to find the \\nminimum points in the valley, we need to go in the opposite direction of the \\ngradient. We update the parameters in the negative gradient direction to \\nminimise the loss. \\n \\nWhere θ is the weight parameter, η is the learning rate, and ∇J(θ;x,y) is the \\ngradient of weight parameter θ \\nTypes of Gradient Descent \\nDifferent types of Gradient descents are \\n\\uf0b7 Batch Gradient Descent or Vanilla Gradient Descent \\n\\uf0b7 Stochastic Gradient Descent'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 19, 'page_label': '20'}, page_content='\\uf0b7 Mini batch Gradient Descent \\nBatch Gradient Descent \\nIn the batch gradient, we use the entire dataset to compute the gradient of \\nthe cost function for each iteration for gradient descent and then update the \\nweights. \\nStochastic Gradient descent \\nStochastic gradient descent, we use a single data point or example to \\ncalculate the gradient and update the weights with every iteration.  \\nWe first need to shuffle the datasets so that we get a completely \\nrandomised dataset. As the datasets are random and weights, are updated \\nfor every single example, an update of the weights and the cost functions \\nwill be noisy jumping all over the place  \\nMini Batch Gradient descent \\nMini-batch gradients is a variation of stochastic gradient descent where \\ninstead of a single training example, a mini-batch of samples are used. \\nMini -batch gradient descent is widely used and converges faster and is \\nmore stable. \\nThe batch size can vary depending upon the dataset. \\nAs we take batches with different samples, it reduces the noise which is a \\nvariance of the weights updates, and that helps to have a more stable \\nconverge faster. \\nQ15: What are autoencoders? \\nAn autoencoder, neural networks that have three layers: \\nAn input layer, a hidden layer which is also known as encoding layer, and a \\ndecoding layer. This network is trained to reconstruct its inputs, which \\nforces the hidden layer to try to learn good representations of the inputs.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 20, 'page_label': '21'}, page_content='An autoencoder neural network is an unsupervised Machine-learning \\nalgorithm that applies backpropagation, setting the target values to be \\nequal to the inputs. An autoencoder is trained to attempts to copy its input \\nto its output. Internally, it has a hidden layer which describes a code used \\nto represent the input. \\n \\nAutoencoder Components: \\nAutoencoders consists of 4 main parts: \\n1- Encoder: In this, the model learns how to reduce the input dimensions \\nand compress the input data into an encoded representation.  \\n2- Bottleneck: In this, the layer that contains the compressed \\nrepresentation of the input data. This is the lowest possible dimension of \\nthe input data. \\n3- Decoder: In this, the model learns how to reconstruct the data from the \\nencod represented to be as close to the original inputs as possible. \\n4- Reconstruction Loss: In this method that measures measure how well \\nthe decoder is performing and how closed the output is related to  the \\noriginal input. \\nTypes of Autoencoders : \\n1. Denoising auto encoder \\n2. Sparse auto encoder \\n3. Variational auto encoder (VAE) \\n4. Contractive auto encoder (CAE)'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 21, 'page_label': '22'}, page_content='Q16: What  is CNN? \\nThis is the simple application of a filter to an input that results  in \\ninactivation. Repeated application of the same filter to input results in a \\nmap of activations called a feature map, indicating the locations and \\nstrength of a detected feature in input, such as an image.  \\nConvolutional layers are the major building blocks which are used in \\nconvolutional neural networks. \\nA covnets is the sequence of layers, and every layer transforms one \\nvolume to another through differentiable functions. \\nDifferent types of layers in CNN: \\nLet’s take an example by running a covnets on of image of dimension s 32 x \\n32 x 3. \\n1. Input Layer: It holds the raw input of image with width 32, height 32 \\nand depth 3. \\n2. Convolution Layer: It computes the output volume by computing dot \\nproducts between all filters and image patches. Suppose we use a \\ntotal of 12 filters for this layer we’ll get output volume of dimension 32 \\nx 32 x 12. \\n3. Activation Function Layer: This layer will apply the element-wise \\nactivation function to the output of the convolution layer. Some  \\nactivation functions are RELU: max(0, x), Sigmoid: 1/(1+e ^-x), Tanh, \\nLeaky RELU, etc. So the volume remains unchanged. Hence output \\nvolume will have dimensions 32 x 32 x 12. \\n4. Pool Layer: This layer is periodically inserted within the covnets, and \\nits main function is to reduce the size of volume which makes the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 22, 'page_label': '23'}, page_content='computation fast reduces memory and also prevents overfitting. Two \\ncommon types of pooling layers are max pooling and average \\npooling. If we use a max pool with 2 x 2 filters and stride 2, the \\nresultant volume will be of dimension 16x16x12. \\n \\n5. Fully-Connected Layer: This layer is a regular neural network layer \\nthat takes input from the previous layer and computes the class \\nscores and outputs the 1-D array of size equal to the number of \\nclasses. \\n \\n \\nQ17: What is pooling, padding, filtering operations on CNN? \\nPooling Layer \\nIt is commonly used to periodically insert a Pooling layer in-between \\nsuccessive Conv layers in a ConvNet architecture. Its function is to \\nprogressively reduce the spatial size of the representation to reduce the \\nnumber of parameters and computation in the network, and hence to also'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 23, 'page_label': '24'}, page_content='control overfitting. The Pooling Layer operates independently on every \\ndepth slice of the input and resizes it spatially, using the MAX operation.  \\n \\nThe most common form is a pooling layer with filters of size 2x2 applied \\nwith a stride of 2 downsamples every depth slice in the input by two along \\nboth width and height, discarding 75% of the activations. Every MAX \\noperation would, in this case, be taking a max over four numbers (little 2x2 \\nregion in some depth slice). The depth dimension remains unchanged.  \\n \\n \\n \\nQ18: What is the Evolution technique of CNN?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 24, 'page_label': '25'}, page_content='It all started with LeNet in 1998 and eventually, after nearly 15 years, lead \\nto groundbreaking models winning the ImageNet Large Scale Visual \\nRecognition Challenge which includes AlexNet in 2012 to Google Net in \\n2014 to ResNet in 2015 to an ensemble of previous models in 2016. In the \\nlast two years, no significant progress has been made, and the new models \\nare an ensemble of previous groundbreaking models. \\n \\nLeNet in 1998 \\nLeNet is a 7-level convolutional network by LeCun in 1998 that classifies \\ndigits and used by several banks to recognise the hand-written numbers on \\ncheques digitised in 32x32 pixel greyscale input images.  \\nAlexNet in 2012 \\nAlexNet: It is considered to be the first paper/ model, which rose the \\ninterest in CNNs when it won the ImageNet challenge in the year 2012. It is \\na deep CNN trained on ImageNet and outperformed all the entries that \\nyear. \\nVGG in 2014 \\nVGG was submitted in the year 2013, and it became a runner up in the \\nImageNet contest in 2014. It is widely used as a simple architecture \\ncompared to AlexNet.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 25, 'page_label': '26'}, page_content=\"GoogleNet in 2014 \\nIn 2014, several great models were developed like VGG, but the winner of \\nthe ImageNet contest was GoogleNet. \\nGoogLeNet proposed a module called the inception modules that includes \\nskipping connections in the network, forming a mini-module, and this \\nmodule is repeated throughout the network. \\nResNet in 2015 \\nThere are 152 layers in the Microsoft ResNet. The authors showed \\nempirically that if you keep on adding layers, the error rate should keep on \\ndecreasing in contrast to “plain nets” we're adding a few layers resulted in \\nhigher training and test errors. \\nQ19: How to initialise biases in deep learning? \\nIt is possible and common to initialise the biases to be zero since the \\nrandom numbers in the weights provide the asymmetry braking . For ReLU \\nnon-linearities, some people like to use small constant value such as 0.01 \\nfor all biases because this ensures that all ReLU units fire in the beginning, \\ntherefore obtain, and propagate some gradient. However, it is unclear if this \\nprovides a consistent improvement (in fact some results seem to indicate s \\nthat this performs worst) and it is more commonly used to use 0 bias \\ninitialisation. \\nQ20: What is learning Rate? \\nLearning Rate \\nThe learning rate controls how much we should adjust the weights \\nconcerning the loss gradient. Learning rates are randomly initialised. \\nLower the values of the learning rate slower will be the convergence to \\nglobal minima. \\nHigher values for the learning rate will not allow the gradient descent to \\nconverge\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 26, 'page_label': '27'}, page_content='Since our goal is to minimise the  function cost to find the optimised value \\nfor weights, we run multiples iteration with different weights and calculate \\nthe cost to arrive at a minimum cost  \\n \\n----------------------------------------------------------------------------------------------------\\n------------------------')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Data Science Interview Preparation(#DAY 04).pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ed9532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 0, 'page_label': '1'}, page_content='DATA SCIENCE \\nINTERVIEW PREPARATION \\n(30 Days of Interview \\nPreparation) \\n \\n# DAY 04'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 1, 'page_label': '2'}, page_content='Q1. What is upsampling and downsampling with examples? \\nThe classification data set with skewed class proportions is called an \\nimbalanced data set. Classes which make up a large proportion of the data \\nsets are called majority classes. Those make up smaller proportions are \\nminority classes. \\nDegree of imbalance Proportion of Minority Class \\n1>> Mild 20-40% of the data set \\n2>> Moderate 1-20% of the data set \\n3>> Extreme <1% of the data set \\nIf we have an imbalanced data set, first try training on the true distribution. \\nIf the model works well and generalises, you are done! If not, try the \\nfollowing up sampling and down sampling technique. \\n1. Up-sampling \\nUpsampling is the process of randomly duplicating observations from the \\nminority class to reinforce its signal. \\nFirst, we will import the resampling module from Scikit-Learn: \\nModule for resampling Python \\n1- From sklearn.utils import resample \\nNext, we will create a new Data Frame with an up-sampled minority class.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 1, 'page_label': '2'}, page_content=\"Here are the steps: \\n1- First, we will separate observations from each class into different Data \\nFrames. \\n2- Next, we will resample the minority class with replacement, setting the \\nnumber of samples to match that of the majority class.  \\n3- Finally, we'll combine the up-sampled minority class Data Frame with the \\noriginal majority class Data Frame. \\n2-Down-sampling \\nDownsampling involves randomly removing observations from the majority \\nclass to prevent its signal from dominating the learning algorithm.  \\nThe process is similar to that of sampling. Here are the steps: \\n1-First, we will separate observations from each class into different Data \\nFrames.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 2, 'page_label': '3'}, page_content='2-Next, we will resample the majority class without replacement, setting the \\nnumber of samples to match that of the minority class.  \\n3-Finally, we will combine the down-sampled majority class Data Frame \\nwith the original minority class Data Frame. \\nQ2. What is the statistical test for data validation with an example,  \\n        Chi-square, ANOVA test, Z statics, T statics, F statics,  \\n Hypothesis Testing? \\nBefore discussing the different statistical test, we need to get a clear \\nunderstanding of what a null hypothesis is. A null hypothesis proposes that \\nhas no significant difference exists in the set of a given observation . \\nNull:  Two samples mean are equal. Alternate: Two samples mean are not \\nequal. \\nFor rejecting the null hypothesis, a test is calculated. Then the test statistic \\nis compared with a critical value, and if found to be greater than the critical \\nvalue, the hypothesis will be rejected. \\nCritical Value:-'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 2, 'page_label': '3'}, page_content='Critical Value:- \\nCritical values are the point beyond which we reject the null hypothesis. \\nCritical value tells us, what is the probability of N number of samples, \\nbelonging to the same distribution. Higher, the critical value which means \\nlower the probability of N number of samples belonging to the same \\ndistribution. \\nCritical values can be used to do hypothesis testing in the following way. \\n1. Calculate test statistic \\n2. Calculate critical values based on the significance level alpha \\n3. Compare test statistics with critical values. \\nIMP-If the test statistic is lower than the critical value, accept the hypothesis \\nor else reject the hypothesis. \\nChi-Square Test:- \\nA chi-square test is used if there is a relationship between two categorical \\nvariables.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 3, 'page_label': '4'}, page_content='Chi-Square test is used to determine whether there is a significant \\ndifference between the expected frequency and the observed frequency in \\none or more categories. Chi-square is also called the non-parametric test \\nas it will not use any parameter \\n \\n \\n \\n2-Anova test:- \\nANOVA, also called an analysis of variance, is used to compare multiples \\n(three or more) samples with a single test. \\nUseful when there are more than three populations. Anova compares the \\nvariance within and between the groups of the population . If the variation is \\nmuch larger than the within variation, the means of different samples will \\nnot be equal. If the between and within variations are approximately the \\nsame size, then there will be no significant difference between sample \\nmeans. Assumptions of ANOVA: 1-All populations involved follow a normal \\ndistribution. 2-All populations have the same variance (or standard \\ndeviation). 3-The samples are randomly selected and independent of one \\nanother.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 3, 'page_label': '4'}, page_content='another. \\nANOVA uses the mean of the samples or the population to reject or \\nsupport the null hypothesis. Hence it is called parametric testing.  \\n3-Z Statics:- \\nIn a z-test, the samples are assumed to be normal distributed. A z score is \\ncalculated with population parameters as “population mean” and \\n“population standard deviation” and it is used to validate a hypothesis that \\nthe sample drawn belongs to the same population. \\nThe statistics used for this hypothesis testing is called z-statistic, the score \\nfor which is calculated as z = (x — μ) / (σ / √n), where x= sample mean μ = \\npopulation mean σ / √n = population standard deviation If the test statistic is \\nlower than the critical value, accept the hypothesis or else reject the \\nhypothesis \\n4- T Statics:- \\nA t-test used to compare the mean of the given samples. Like  z-test,  t-test \\nalso assumed a normal distribution of the samples. A t-test is used when \\nthe population parameters (mean and standard deviation) are unknown.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 4, 'page_label': '5'}, page_content='There are three versions of t-test \\n1. Independent samples t-test which compare means for two groups \\n2. Paired sample t-test which compares mean from the same group at \\ndifferent times \\n3. Sample t-test, which tests the mean of the single group against the \\nknown mean. The statistic for  hypothesis testing is called t-statistic, \\nthe score for which is calculated as t = (x1 — x2) / (σ / √n1 + σ / √n2), \\nwhere \\n              x1 =  It is mean of sample A, x2 = mean of sample B,  \\n              n1 = size of sample 1 n2 = size of    sample 2 \\n \\n \\n \\n5- F Statics:- \\nThe F-test is designed to test if the two population variances are equal. It \\ncompares the ratio of the two variances. Therefore, if the variances are \\nequal, then the ratio of the variances will be 1. \\nThe F-distribution is the ratio of two independent chi-square variables \\ndivided by their respective degrees of freedom. \\nF = s1^2 / s2^2 and where s1^2 > s2^2.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 4, 'page_label': '5'}, page_content='If the null hypothesis is true, then the F test-statistic given above can be \\nsimplified. This ratio of sample variances will be tested statistic used. If the \\nnull hypothesis is false, then we will reject the null hypothesis that the ratio \\nwas equal to 1 and our assumption that they were equal. \\n \\nQ3. What is the Central limit theorem? \\nCentral Limit Theorem \\nDefinition: The theorem states that as the size of the sample increases, the \\ndistribution of the mean across multiple samples will approximate a \\nGaussian distribution (Normal). Generally, sample sizes equal to or greater \\nthan 30 are consider sufficient for the CLT to hold. It means that the \\ndistribution of the sample means is normally distributed. The average of the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 5, 'page_label': '6'}, page_content='sample means will be equal to the population mean. This is the key aspect \\nof the theorem. \\nAssumptions: \\n1. The data must follow the randomization condition. It must be sampled \\nrandomly \\n2. Samples should be independent of each other. One sample should \\nnot influence the other samples \\n3. Sample size should be no more than 10% of the population when \\nsampling is done without replacement \\n4. The sample size should be sufficiently large. The mean of the sample \\nmeans is denoted as: \\nµ X̄ = µ \\nWhere, \\nµ X̄ = Mean of the sample means µ= Population mean and, the standard \\ndeviation of the sample mean is denoted as: \\nσ X̄ = σ/sqrt(n) \\nWhere, \\nσ X̄ = Standard deviation of the sample mean σ = Population standard \\ndeviation n = sample size \\nA sufficiently large sample size can predict the characteristics of a \\npopulation accurately. For Example, we shall take a uniformly distributed \\ndata: \\nRandomly distributed data: Even for a randomly (Exponential) distributed'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 5, 'page_label': '6'}, page_content='data the plot of the means is normally distributed. \\nThe advantage of CLT is that we need not worry about the actual data \\nsince the means of it will always be normally distributed. With this, we can \\ncreate component intervals, perform T-tests and ANOVA tests from the \\ngiven samples. \\n \\nQ4. What is the correlation and coefficient? \\nWhat is the Correlation Coefficient? \\nThe correlation coefficient is a statistical measure that calculates the \\nstrength of the relationship between the relative movements of two'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 6, 'page_label': '7'}, page_content='variables. We use it to measure both the strength and direction of a linear \\nrelationship between two variables the values range between -1.0 and 1.0. \\nA calculated number greater than 1.0 or less than -1.0 means that there \\nwas an error in the correlation measurement. A correlation of -1.0 shows a \\nperfect negative correlation, while a correlation of 1.0 shows a perfect \\npositive correlation. \\n \\n \\n \\nCorrelation coefficient formulas are used to find how strong a relationship is \\nbetween data. The formulas return a value between -1 and 1, where: \\n1 indicates a strong positive relationship. -1 indicates a strong negative \\nrelationship. A result of zero indicates no relationship at all.  \\n \\nMeaning \\n1. A correlation coefficient of 1 means that for every positive increase in \\none variable, there is a positive increase in a fixed proportion in the \\nother. For example, shoe sizes go up in (almost) perfect correlation \\nwith foot length.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 6, 'page_label': '7'}, page_content='with foot length. \\n2. A correlation coefficient of -1 means that for every positive increase in \\none variable, there is a negative decrease of a fixed proportion in the \\nother. For example, the amount of gas in a tank decreases in  (almost) \\nperfect correlation with speed. \\n3. Zero means that for every increase, there isn’t a positive or negative \\nincrease. The two just aren’t related.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 7, 'page_label': '8'}, page_content='What is a Negative Correlation? \\nNegative correlation is a relationship between two variables in which one \\nvariable increases as the other decreases, and vice versa. In statistics, a \\nperfect negative correlation is represented by the value -1. Negative \\ncorrelation or inverse correlation is a relationship between two variables \\nwhereby they move in opposite directions. If variables X and Y have a \\nnegative correlation (or are negatively correlated), as X increases in value, \\nY will decrease; similarly, if X decreases in value, Y will increase.  \\nWhat Is Positive Correlation? \\nPositive correlation is a relationship between two variables in which both \\nvariables move in tandem—that is, in the same direction. A positive \\ncorrelation exists when one variable decreases as the other variable \\ndecreases or one variable increases while the other increases.  \\n \\n \\n \\nWe use the correlation coefficient to measure the strength and direction of'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 7, 'page_label': '8'}, page_content='the linear relationship between two numerical variables X and Y. The \\ncorrelation coefficient for a sample of data is denoted by r.  \\nPearson Correlation Coefficient \\nPearson is the most widely used correlation coefficient. Pearson correlation \\nmeasures the linear association between continuous variables. In other \\nwords, this coefficient quantifies the degree to which a relationship between \\ntwo variables can be described by a line. Formula developed by Karl \\nPearson over 120 years ago is still the most widely used today. The \\nformula for the correlation (r) is'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 8, 'page_label': '9'}, page_content='Where n is the number of pairs of data; \\nAre the sample means of all the x-values and all the y-values, respectively; \\nand sx and sy are the sample standard deviations of all the x- and y-values, \\nrespectively. \\n1. Find the mean of all the x-values and mean of all y-values. \\n2. Find the standard deviation of all the x-values (call it sx) and the \\nstandard deviation of all the y-values (call it sy). For example, to find \\nsx, you would use the following equation: \\n3. For each of the n pairs (x, y) in the data set, take \\n4. Add up the n results from Step 3. \\n5. Divide the sum by sx ∗ sy. \\n6. Divide the result by n – 1, where n is the number of (x, y) pairs. (It’s \\nthe same as multiplying by 1 over n – 1.) This gives you the \\ncorrelation, r. \\n \\nQ5: What is the difference between machine learning and deep  \\n       learning? \\nMachine Learning | deep learning \\nMachine Learning is a technique to learn from that data and then apply wha'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 8, 'page_label': '9'}, page_content='t has been learnt to make an informed decision | The main difference betwe\\nen deep and machine learning is, machine learning models become better \\nprogressively but the model still needs some guidance. If a machine-\\nlearning model returns an inaccurate prediction then the programmer need\\ns to fix that problem explicitly but in the case of deep learning, the model do\\nes it by himself.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 9, 'page_label': '10'}, page_content='>Machine Learning can perform well with small size data also | Deep Learn\\ning does not perform as good with smaller datasets. \\n>Machine learning can work on some low-\\nend machines also  | Deep Learning involves many matrix multiplication op\\nerations which are better suited for GPUs \\n>Features need to be identified and extracted as per the domain before pu\\nshing them to the algorithm | Deep learning algorithms try to learn high-\\nlevel features from data. \\n>It is generally recommended to break the problem into smaller chunks, sol\\nve them and then combine the results | It generally focusses on solving the \\nproblem end to end \\n>Training time is comparatively less | Training time is comparatively more \\n>Results are more interpretable | Results Maybe more accurate but less int\\nerpretable \\n> No use of Neural networks | uses neural networks \\n> Solves comparatively less complex problems | Solves more complex prob\\nlems. \\nQ6: What is perceptron and how it is related to human neurons?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 9, 'page_label': '10'}, page_content='If we focus on the structure of a biological neuron, it has dendrites, which \\nare used to receive inputs. These inputs are summed in the cell body and \\nusing the Axon it is passed on to the next biological neuron as shown \\nbelow.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 10, 'page_label': '11'}, page_content='Dendrite: Receives signals from other neurons \\nCell Body: Sums all the inputs \\nAxon: It is used to transmit signals to the other cells \\nSimilarly, a perceptron receives multiple inputs, applies various \\ntransformations and functions and provides an output. A Perceptron is a \\nlinear model used for binary classification. It models a neuron, which has a \\nset of inputs, each of which is given a specific weight. The neuron \\ncomputes some function on these weighted inputs and gives the output.  \\n \\n \\nQ7: Why deep learning is better than machine learning? \\nThough traditional ML algorithms solve a lot of our cases, they are not \\nuseful while working with high dimensional data that is where we have a \\nlarge number of inputs and outputs. For example, in the case of \\nhandwriting recognition, we have a large amount of input where we will \\nhave different types of inputs associated with different types of handwriting.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 11, 'page_label': '12'}, page_content='The second major challenge is to tell the computer what are the features it \\nshould look for that will play an important role in predicting the outcome a s \\nwell as to achieve better accuracy while doing so. \\nQ8: What kind of problem can be solved by using deep learning?  \\nDeep Learning is a branch of Machine Learning, which is used to solve \\nproblems in a way that mimics the human way of solving problems. \\nExamples: \\n\\uf0b7 Image recognition \\n\\uf0b7 Object Detection \\n\\uf0b7 Natural Language processing- Translation, Sentence formations, text \\nto speech, speech to text \\n\\uf0b7 understand the semantics of actions \\nQ9: List down all the activation function using mathematical    \\n       Expression and example. What is the activation function? \\nActivation functions are very important for an Artificial Neural Network to \\nlearn and make sense of something complicated and the Non-linear \\ncomplex functional mappings between the inputs and response variable.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 11, 'page_label': '12'}, page_content='They introduce non-linear properties to our Network. Their main purposes \\nare to convert an input signal of a node in an A-NN to an output signal. \\nSo why do we need Non-Linearities?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 12, 'page_label': '13'}, page_content='Non-linear functions are those, which have a degree more than one, and \\nthey have a curvature when we plot a Non-Linear function. Now we need a \\nNeural Network Model to learn and represent almost anything and any \\narbitrary complex function, which maps inputs to outputs. Neural-Networks \\nare considered Universal Function Approximations. It means that they can \\ncompute and learn any function at all. \\nMost popular types of Activation functions - \\n\\uf0b7 Sigmoid or Logistic \\n\\uf0b7 Tanh — Hyperbolic tangent \\n\\uf0b7 ReLu -Rectified linear units \\nSigmoid Activation function: It is a activation function of form f(x) = 1 / 1 \\n+ exp(-x) . Its Range is between 0 and 1. It is an S-shaped curve. It is easy \\nto understand. \\n \\nHyperbolic Tangent function- Tanh : It’s mathematical formula is f(x) = 1 \\n— exp(-2x) / 1 + exp(-2x). Now it’s the output is zero centred because its \\nrange in between -1 to 1 i.e. -1 < output < 1 . Hence optimisation is easier'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 12, 'page_label': '13'}, page_content='in this method; Hence in practice, it is always preferred over Sigmoid \\nfunction.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 13, 'page_label': '14'}, page_content='ReLu- Rectified Linear units: It has become more popular in the past \\ncouple of years. It was recently proved that it has six times improvement in \\nconvergence from Tanh function. It’s R(x) = max (0,x) i.e. if x < 0 , R(x) = 0 \\nand if x >= 0 , R(x) = x. Hence as seen that mathematical form of this \\nfunction, we can see that it is very simple and efficient. Many times in \\nMachine learning and computer science we notice that most simple and \\nconsistent techniques and methods are only preferred and  are the best. \\nHence, it avoids and rectifies the vanishing gradient problem. Almost all the \\ndeep learning Models use ReLu nowadays. \\n \\n \\nQ10: Detail explanation about gradient decent using example and  \\n       Mathematical expression?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 14, 'page_label': '15'}, page_content='Gradient descent is an optimisation algorithm used to minimize some \\nfunction by iteratively moving in the direction of steepest descent as \\ndefined by negative of the gradient. In machine learning, we used gradient \\ndescent to update the parameters of our model. Parameters refer t o \\ncoefficients in the Linear Regression and weights in neural networks.  \\n \\nThe size of these steps called the learning rate. With the high learning rate, \\nwe can cover more ground each step, but we risk overshooting the lower \\npoint since the slope of the hill is constantly changing. With a very lower \\nlearning rate, we can confidently move in the direction of the negativ e \\ngradient because we are recalculating it so frequently. The Lower learning \\nrate is more precise, but calculating the gradient is time -consuming, so it \\nwill take a very large time to get to the bottom. \\nMath \\nNow let’s run gradient descent using new cost function. There are two'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 14, 'page_label': '15'}, page_content='parameters in cost function we can control: m (weight) and b (bias). Since \\nwe need to consider that the impact each one has on the final prediction, \\nwe need to use partial derivatives. We calculate the partial derivative of the \\ncost function concerning each parameter and store the results in a \\ngradient. \\nMath \\nGiven the cost function:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 15, 'page_label': '16'}, page_content='To solve for the gradient, we iterate by our data points using our new m \\nand b values and compute the partial derivatives. This new gradient te lls us \\nabout the slope of the cost function at our current position (current \\nparameter values) and the directions we should move to update our \\nparameters. The learning rate controls the size of our update. \\nQ11: What is backward propagation?  \\nBack-propagation is the essence of the neural net training and this \\nmethod of fine-tuning the weights of a neural net based on the errors rate \\nobtained in the previous epoch. Proper tuning of the weights allows us to \\nreduce error rates and to make the model reliable by increasing its \\ngeneralisation. \\nBackpropagation is a short form of \"backward propagation of errors.\" This \\nis the standard method of training artificial neural networks. This helps to \\ncalculate the gradient of a loss function with respects to all the weights in \\nthe network.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 16, 'page_label': '17'}, page_content='Most prominent advantages of Backpropagation are: \\n\\uf0b7 Backpropagation is the fast, simple and easy to program. \\n\\uf0b7 It has no parameters to tune apart from the numbers of input . \\n\\uf0b7 It is the flexible method as it does not require prior knowledge about \\nthe network \\n\\uf0b7 It is the standard method that generally works well. \\n\\uf0b7 It does not need any special mentions of the features of the function \\nto be learned. \\n \\n \\n \\nQ12: How we assign weights in deep learning? \\nWe already know that in a neural network, weights are usually initialised \\nrandomly and that kind of initialisation takes a fair/significant amount of \\nrepetitions to converge to the least loss and reach the ideal wei ght matrix. \\nThe problem is, that kind of initialisation is prone to vanishing or exploding \\ngradient problems.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 17, 'page_label': '18'}, page_content='General ways to make it initialise better weights: \\nReLu activation function in the deep nets. \\n1. Generate a random sample of weights from a Gaussian \\ndistribution having mean 0 and a standard deviation of 1.  \\n2. Multiply the sample with the square root of (2/ni). Where ni is the \\nnumber of input units for that layer. \\nb) Likewise, if you’re using Tanh activation function : \\n1. Generate a random sample of weights from a Gaussian distribution \\nhaving mean 0 and a standard deviation of 1. \\n2. Multiply the sample with the square root of (1/ni) where ni is several \\ninput units for that layer. \\n \\nQ13: What is optimiser is deep learning, and which one is the best? \\nDeep learning is an iterative process. With so many hyperparameters to \\ntune or methods to try, it is important to be able to train models fast, to \\nquickly complete the iterative cycle. This is the key to increase the speed \\nand efficiency of a machine learning team.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 17, 'page_label': '18'}, page_content='Hence the importance of optimisation algorithms such as stochastic \\ngradient descent, min-batch gradient descent, gradient descent with \\nmomentum and the Adam optimiser. \\nAdam optimiser is the best one. \\nGiven an algorithm f(x), it helps in either minimisation or maximisation of \\nthe value of f(x). In this context of deep learning, we use optimisation \\nalgorithms to train the neural network by optimising the cost function J.  \\nThe cost function is defined as:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 18, 'page_label': '19'}, page_content='The value of the cost function J is the mean of the loss L between the \\npredicted value y’ and actual value y. The value y” is obtained during the \\nforward propagation step and makes use of the Weights W and biases b of \\nthe network. With the help of optimisation algorithms, we minimise the \\nvalue of Cost Function J  by updating the values of trainable \\nparameters W and b. \\nQ14: What is gradient descent, mini-batch gradient descent, batch  \\n         gradient decent, stochastic gradient decent and adam?  \\nGradient Descent \\nit is an iterative machine learning optimisation algorithm to reduce the cost \\nfunction, and help models to make accurate predictions. \\nGradient indicates the direction of increase. As we want to find the \\nminimum points in the valley, we need to go in the opposite direction of the \\ngradient. We update the parameters in the negative gradient direction to \\nminimise the loss. \\n \\nWhere θ is the weight parameter, η is the learning rate, and ∇J(θ;x,y) is the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 18, 'page_label': '19'}, page_content='gradient of weight parameter θ \\nTypes of Gradient Descent \\nDifferent types of Gradient descents are \\n\\uf0b7 Batch Gradient Descent or Vanilla Gradient Descent \\n\\uf0b7 Stochastic Gradient Descent'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 19, 'page_label': '20'}, page_content='\\uf0b7 Mini batch Gradient Descent \\nBatch Gradient Descent \\nIn the batch gradient, we use the entire dataset to compute the gradient of \\nthe cost function for each iteration for gradient descent and then update the \\nweights. \\nStochastic Gradient descent \\nStochastic gradient descent, we use a single data point or example to \\ncalculate the gradient and update the weights with every iteration.  \\nWe first need to shuffle the datasets so that we get a completely \\nrandomised dataset. As the datasets are random and weights, are updated \\nfor every single example, an update of the weights and the cost functions \\nwill be noisy jumping all over the place  \\nMini Batch Gradient descent \\nMini-batch gradients is a variation of stochastic gradient descent where \\ninstead of a single training example, a mini-batch of samples are used. \\nMini -batch gradient descent is widely used and converges faster and is \\nmore stable. \\nThe batch size can vary depending upon the dataset.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 19, 'page_label': '20'}, page_content='As we take batches with different samples, it reduces the noise which is a \\nvariance of the weights updates, and that helps to have a more stable \\nconverge faster. \\nQ15: What are autoencoders? \\nAn autoencoder, neural networks that have three layers: \\nAn input layer, a hidden layer which is also known as encoding layer, and a \\ndecoding layer. This network is trained to reconstruct its inputs, which \\nforces the hidden layer to try to learn good representations of the inputs.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 20, 'page_label': '21'}, page_content='An autoencoder neural network is an unsupervised Machine-learning \\nalgorithm that applies backpropagation, setting the target values to be \\nequal to the inputs. An autoencoder is trained to attempts to copy its input \\nto its output. Internally, it has a hidden layer which describes a code used \\nto represent the input. \\n \\nAutoencoder Components: \\nAutoencoders consists of 4 main parts: \\n1- Encoder: In this, the model learns how to reduce the input dimensions \\nand compress the input data into an encoded representation.  \\n2- Bottleneck: In this, the layer that contains the compressed \\nrepresentation of the input data. This is the lowest possible dimension of \\nthe input data. \\n3- Decoder: In this, the model learns how to reconstruct the data from the \\nencod represented to be as close to the original inputs as possible. \\n4- Reconstruction Loss: In this method that measures measure how well \\nthe decoder is performing and how closed the output is related to  the \\noriginal input.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 20, 'page_label': '21'}, page_content='original input. \\nTypes of Autoencoders : \\n1. Denoising auto encoder \\n2. Sparse auto encoder \\n3. Variational auto encoder (VAE) \\n4. Contractive auto encoder (CAE)'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 21, 'page_label': '22'}, page_content='Q16: What  is CNN? \\nThis is the simple application of a filter to an input that results  in \\ninactivation. Repeated application of the same filter to input results in a \\nmap of activations called a feature map, indicating the locations and \\nstrength of a detected feature in input, such as an image.  \\nConvolutional layers are the major building blocks which are used in \\nconvolutional neural networks. \\nA covnets is the sequence of layers, and every layer transforms one \\nvolume to another through differentiable functions. \\nDifferent types of layers in CNN: \\nLet’s take an example by running a covnets on of image of dimension s 32 x \\n32 x 3. \\n1. Input Layer: It holds the raw input of image with width 32, height 32 \\nand depth 3. \\n2. Convolution Layer: It computes the output volume by computing dot \\nproducts between all filters and image patches. Suppose we use a \\ntotal of 12 filters for this layer we’ll get output volume of dimension 32 \\nx 32 x 12.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 21, 'page_label': '22'}, page_content='x 32 x 12. \\n3. Activation Function Layer: This layer will apply the element-wise \\nactivation function to the output of the convolution layer. Some  \\nactivation functions are RELU: max(0, x), Sigmoid: 1/(1+e ^-x), Tanh, \\nLeaky RELU, etc. So the volume remains unchanged. Hence output \\nvolume will have dimensions 32 x 32 x 12. \\n4. Pool Layer: This layer is periodically inserted within the covnets, and \\nits main function is to reduce the size of volume which makes the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 22, 'page_label': '23'}, page_content='computation fast reduces memory and also prevents overfitting. Two \\ncommon types of pooling layers are max pooling and average \\npooling. If we use a max pool with 2 x 2 filters and stride 2, the \\nresultant volume will be of dimension 16x16x12. \\n \\n5. Fully-Connected Layer: This layer is a regular neural network layer \\nthat takes input from the previous layer and computes the class \\nscores and outputs the 1-D array of size equal to the number of \\nclasses. \\n \\n \\nQ17: What is pooling, padding, filtering operations on CNN? \\nPooling Layer \\nIt is commonly used to periodically insert a Pooling layer in-between \\nsuccessive Conv layers in a ConvNet architecture. Its function is to \\nprogressively reduce the spatial size of the representation to reduce the \\nnumber of parameters and computation in the network, and hence to also'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 23, 'page_label': '24'}, page_content='control overfitting. The Pooling Layer operates independently on every \\ndepth slice of the input and resizes it spatially, using the MAX operation.  \\n \\nThe most common form is a pooling layer with filters of size 2x2 applied \\nwith a stride of 2 downsamples every depth slice in the input by two along \\nboth width and height, discarding 75% of the activations. Every MAX \\noperation would, in this case, be taking a max over four numbers (little 2x2 \\nregion in some depth slice). The depth dimension remains unchanged.  \\n \\n \\n \\nQ18: What is the Evolution technique of CNN?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 24, 'page_label': '25'}, page_content='It all started with LeNet in 1998 and eventually, after nearly 15 years, lead \\nto groundbreaking models winning the ImageNet Large Scale Visual \\nRecognition Challenge which includes AlexNet in 2012 to Google Net in \\n2014 to ResNet in 2015 to an ensemble of previous models in 2016. In the \\nlast two years, no significant progress has been made, and the new models \\nare an ensemble of previous groundbreaking models. \\n \\nLeNet in 1998 \\nLeNet is a 7-level convolutional network by LeCun in 1998 that classifies \\ndigits and used by several banks to recognise the hand-written numbers on \\ncheques digitised in 32x32 pixel greyscale input images.  \\nAlexNet in 2012 \\nAlexNet: It is considered to be the first paper/ model, which rose the \\ninterest in CNNs when it won the ImageNet challenge in the year 2012. It is \\na deep CNN trained on ImageNet and outperformed all the entries that \\nyear. \\nVGG in 2014 \\nVGG was submitted in the year 2013, and it became a runner up in the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 24, 'page_label': '25'}, page_content='ImageNet contest in 2014. It is widely used as a simple architecture \\ncompared to AlexNet.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 25, 'page_label': '26'}, page_content=\"GoogleNet in 2014 \\nIn 2014, several great models were developed like VGG, but the winner of \\nthe ImageNet contest was GoogleNet. \\nGoogLeNet proposed a module called the inception modules that includes \\nskipping connections in the network, forming a mini-module, and this \\nmodule is repeated throughout the network. \\nResNet in 2015 \\nThere are 152 layers in the Microsoft ResNet. The authors showed \\nempirically that if you keep on adding layers, the error rate should keep on \\ndecreasing in contrast to “plain nets” we're adding a few layers resulted in \\nhigher training and test errors. \\nQ19: How to initialise biases in deep learning? \\nIt is possible and common to initialise the biases to be zero since the \\nrandom numbers in the weights provide the asymmetry braking . For ReLU \\nnon-linearities, some people like to use small constant value such as 0.01 \\nfor all biases because this ensures that all ReLU units fire in the beginning,\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 25, 'page_label': '26'}, page_content='therefore obtain, and propagate some gradient. However, it is unclear if this \\nprovides a consistent improvement (in fact some results seem to indicate s \\nthat this performs worst) and it is more commonly used to use 0 bias \\ninitialisation. \\nQ20: What is learning Rate? \\nLearning Rate \\nThe learning rate controls how much we should adjust the weights \\nconcerning the loss gradient. Learning rates are randomly initialised. \\nLower the values of the learning rate slower will be the convergence to \\nglobal minima. \\nHigher values for the learning rate will not allow the gradient descent to \\nconverge'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 26, 'page_label': '27'}, page_content='Since our goal is to minimise the  function cost to find the optimised value \\nfor weights, we run multiples iteration with different weights and calculate \\nthe cost to arrive at a minimum cost  \\n \\n----------------------------------------------------------------------------------------------------\\n------------------------')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e32fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 0, 'page_label': '1'}, page_content='DATA SCIENCE \\nINTERVIEW PREPARATION \\n(30 Days of Interview \\nPreparation) \\n \\n# DAY 04'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 1, 'page_label': '2'}, page_content='Q1. What is upsampling and downsampling with examples? \\nThe classification data set with skewed class proportions is called an \\nimbalanced data set. Classes which make up a large proportion of the data \\nsets are called majority classes. Those make up smaller proportions are \\nminority classes. \\nDegree of imbalance Proportion of Minority Class \\n1>> Mild 20-40% of the data set \\n2>> Moderate 1-20% of the data set \\n3>> Extreme <1% of the data set \\nIf we have an imbalanced data set, first try training on the true distribution. \\nIf the model works well and generalises, you are done! If not, try the \\nfollowing up sampling and down sampling technique. \\n1. Up-sampling \\nUpsampling is the process of randomly duplicating observations from the \\nminority class to reinforce its signal. \\nFirst, we will import the resampling module from Scikit-Learn: \\nModule for resampling Python \\n1- From sklearn.utils import resample \\nNext, we will create a new Data Frame with an up-sampled minority class.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 1, 'page_label': '2'}, page_content=\"Here are the steps: \\n1- First, we will separate observations from each class into different Data \\nFrames. \\n2- Next, we will resample the minority class with replacement, setting the \\nnumber of samples to match that of the majority class.  \\n3- Finally, we'll combine the up-sampled minority class Data Frame with the \\noriginal majority class Data Frame. \\n2-Down-sampling \\nDownsampling involves randomly removing observations from the majority \\nclass to prevent its signal from dominating the learning algorithm.  \\nThe process is similar to that of sampling. Here are the steps: \\n1-First, we will separate observations from each class into different Data \\nFrames.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 2, 'page_label': '3'}, page_content='2-Next, we will resample the majority class without replacement, setting the \\nnumber of samples to match that of the minority class.  \\n3-Finally, we will combine the down-sampled majority class Data Frame \\nwith the original minority class Data Frame. \\nQ2. What is the statistical test for data validation with an example,  \\n        Chi-square, ANOVA test, Z statics, T statics, F statics,  \\n Hypothesis Testing? \\nBefore discussing the different statistical test, we need to get a clear \\nunderstanding of what a null hypothesis is. A null hypothesis proposes that \\nhas no significant difference exists in the set of a given observation . \\nNull:  Two samples mean are equal. Alternate: Two samples mean are not \\nequal. \\nFor rejecting the null hypothesis, a test is calculated. Then the test statistic \\nis compared with a critical value, and if found to be greater than the critical \\nvalue, the hypothesis will be rejected. \\nCritical Value:-'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 2, 'page_label': '3'}, page_content='Critical Value:- \\nCritical values are the point beyond which we reject the null hypothesis. \\nCritical value tells us, what is the probability of N number of samples, \\nbelonging to the same distribution. Higher, the critical value which means \\nlower the probability of N number of samples belonging to the same \\ndistribution. \\nCritical values can be used to do hypothesis testing in the following way. \\n1. Calculate test statistic \\n2. Calculate critical values based on the significance level alpha \\n3. Compare test statistics with critical values. \\nIMP-If the test statistic is lower than the critical value, accept the hypothesis \\nor else reject the hypothesis. \\nChi-Square Test:- \\nA chi-square test is used if there is a relationship between two categorical \\nvariables.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 3, 'page_label': '4'}, page_content='Chi-Square test is used to determine whether there is a significant \\ndifference between the expected frequency and the observed frequency in \\none or more categories. Chi-square is also called the non-parametric test \\nas it will not use any parameter \\n \\n \\n \\n2-Anova test:- \\nANOVA, also called an analysis of variance, is used to compare multiples \\n(three or more) samples with a single test. \\nUseful when there are more than three populations. Anova compares the \\nvariance within and between the groups of the population . If the variation is \\nmuch larger than the within variation, the means of different samples will \\nnot be equal. If the between and within variations are approximately the \\nsame size, then there will be no significant difference between sample \\nmeans. Assumptions of ANOVA: 1-All populations involved follow a normal \\ndistribution. 2-All populations have the same variance (or standard \\ndeviation). 3-The samples are randomly selected and independent of one \\nanother.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 3, 'page_label': '4'}, page_content='another. \\nANOVA uses the mean of the samples or the population to reject or \\nsupport the null hypothesis. Hence it is called parametric testing.  \\n3-Z Statics:- \\nIn a z-test, the samples are assumed to be normal distributed. A z score is \\ncalculated with population parameters as “population mean” and \\n“population standard deviation” and it is used to validate a hypothesis that \\nthe sample drawn belongs to the same population. \\nThe statistics used for this hypothesis testing is called z-statistic, the score \\nfor which is calculated as z = (x — μ) / (σ / √n), where x= sample mean μ = \\npopulation mean σ / √n = population standard deviation If the test statistic is \\nlower than the critical value, accept the hypothesis or else reject the \\nhypothesis \\n4- T Statics:- \\nA t-test used to compare the mean of the given samples. Like  z-test,  t-test \\nalso assumed a normal distribution of the samples. A t-test is used when \\nthe population parameters (mean and standard deviation) are unknown.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 4, 'page_label': '5'}, page_content='There are three versions of t-test \\n1. Independent samples t-test which compare means for two groups \\n2. Paired sample t-test which compares mean from the same group at \\ndifferent times \\n3. Sample t-test, which tests the mean of the single group against the \\nknown mean. The statistic for  hypothesis testing is called t-statistic, \\nthe score for which is calculated as t = (x1 — x2) / (σ / √n1 + σ / √n2), \\nwhere \\n              x1 =  It is mean of sample A, x2 = mean of sample B,  \\n              n1 = size of sample 1 n2 = size of    sample 2 \\n \\n \\n \\n5- F Statics:- \\nThe F-test is designed to test if the two population variances are equal. It \\ncompares the ratio of the two variances. Therefore, if the variances are \\nequal, then the ratio of the variances will be 1. \\nThe F-distribution is the ratio of two independent chi-square variables \\ndivided by their respective degrees of freedom. \\nF = s1^2 / s2^2 and where s1^2 > s2^2.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 4, 'page_label': '5'}, page_content='If the null hypothesis is true, then the F test-statistic given above can be \\nsimplified. This ratio of sample variances will be tested statistic used. If the \\nnull hypothesis is false, then we will reject the null hypothesis that the ratio \\nwas equal to 1 and our assumption that they were equal. \\n \\nQ3. What is the Central limit theorem? \\nCentral Limit Theorem \\nDefinition: The theorem states that as the size of the sample increases, the \\ndistribution of the mean across multiple samples will approximate a \\nGaussian distribution (Normal). Generally, sample sizes equal to or greater \\nthan 30 are consider sufficient for the CLT to hold. It means that the \\ndistribution of the sample means is normally distributed. The average of the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 5, 'page_label': '6'}, page_content='sample means will be equal to the population mean. This is the key aspect \\nof the theorem. \\nAssumptions: \\n1. The data must follow the randomization condition. It must be sampled \\nrandomly \\n2. Samples should be independent of each other. One sample should \\nnot influence the other samples \\n3. Sample size should be no more than 10% of the population when \\nsampling is done without replacement \\n4. The sample size should be sufficiently large. The mean of the sample \\nmeans is denoted as: \\nµ X̄ = µ \\nWhere, \\nµ X̄ = Mean of the sample means µ= Population mean and, the standard \\ndeviation of the sample mean is denoted as: \\nσ X̄ = σ/sqrt(n) \\nWhere, \\nσ X̄ = Standard deviation of the sample mean σ = Population standard \\ndeviation n = sample size \\nA sufficiently large sample size can predict the characteristics of a \\npopulation accurately. For Example, we shall take a uniformly distributed \\ndata: \\nRandomly distributed data: Even for a randomly (Exponential) distributed'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 5, 'page_label': '6'}, page_content='data the plot of the means is normally distributed. \\nThe advantage of CLT is that we need not worry about the actual data \\nsince the means of it will always be normally distributed. With this, we can \\ncreate component intervals, perform T-tests and ANOVA tests from the \\ngiven samples. \\n \\nQ4. What is the correlation and coefficient? \\nWhat is the Correlation Coefficient? \\nThe correlation coefficient is a statistical measure that calculates the \\nstrength of the relationship between the relative movements of two'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 6, 'page_label': '7'}, page_content='variables. We use it to measure both the strength and direction of a linear \\nrelationship between two variables the values range between -1.0 and 1.0. \\nA calculated number greater than 1.0 or less than -1.0 means that there \\nwas an error in the correlation measurement. A correlation of -1.0 shows a \\nperfect negative correlation, while a correlation of 1.0 shows a perfect \\npositive correlation. \\n \\n \\n \\nCorrelation coefficient formulas are used to find how strong a relationship is \\nbetween data. The formulas return a value between -1 and 1, where: \\n1 indicates a strong positive relationship. -1 indicates a strong negative \\nrelationship. A result of zero indicates no relationship at all.  \\n \\nMeaning \\n1. A correlation coefficient of 1 means that for every positive increase in \\none variable, there is a positive increase in a fixed proportion in the \\nother. For example, shoe sizes go up in (almost) perfect correlation \\nwith foot length.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 6, 'page_label': '7'}, page_content='with foot length. \\n2. A correlation coefficient of -1 means that for every positive increase in \\none variable, there is a negative decrease of a fixed proportion in the \\nother. For example, the amount of gas in a tank decreases in  (almost) \\nperfect correlation with speed. \\n3. Zero means that for every increase, there isn’t a positive or negative \\nincrease. The two just aren’t related.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 7, 'page_label': '8'}, page_content='What is a Negative Correlation? \\nNegative correlation is a relationship between two variables in which one \\nvariable increases as the other decreases, and vice versa. In statistics, a \\nperfect negative correlation is represented by the value -1. Negative \\ncorrelation or inverse correlation is a relationship between two variables \\nwhereby they move in opposite directions. If variables X and Y have a \\nnegative correlation (or are negatively correlated), as X increases in value, \\nY will decrease; similarly, if X decreases in value, Y will increase.  \\nWhat Is Positive Correlation? \\nPositive correlation is a relationship between two variables in which both \\nvariables move in tandem—that is, in the same direction. A positive \\ncorrelation exists when one variable decreases as the other variable \\ndecreases or one variable increases while the other increases.  \\n \\n \\n \\nWe use the correlation coefficient to measure the strength and direction of'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 7, 'page_label': '8'}, page_content='the linear relationship between two numerical variables X and Y. The \\ncorrelation coefficient for a sample of data is denoted by r.  \\nPearson Correlation Coefficient \\nPearson is the most widely used correlation coefficient. Pearson correlation \\nmeasures the linear association between continuous variables. In other \\nwords, this coefficient quantifies the degree to which a relationship between \\ntwo variables can be described by a line. Formula developed by Karl \\nPearson over 120 years ago is still the most widely used today. The \\nformula for the correlation (r) is'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 8, 'page_label': '9'}, page_content='Where n is the number of pairs of data; \\nAre the sample means of all the x-values and all the y-values, respectively; \\nand sx and sy are the sample standard deviations of all the x- and y-values, \\nrespectively. \\n1. Find the mean of all the x-values and mean of all y-values. \\n2. Find the standard deviation of all the x-values (call it sx) and the \\nstandard deviation of all the y-values (call it sy). For example, to find \\nsx, you would use the following equation: \\n3. For each of the n pairs (x, y) in the data set, take \\n4. Add up the n results from Step 3. \\n5. Divide the sum by sx ∗ sy. \\n6. Divide the result by n – 1, where n is the number of (x, y) pairs. (It’s \\nthe same as multiplying by 1 over n – 1.) This gives you the \\ncorrelation, r. \\n \\nQ5: What is the difference between machine learning and deep  \\n       learning? \\nMachine Learning | deep learning \\nMachine Learning is a technique to learn from that data and then apply wha'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 8, 'page_label': '9'}, page_content='t has been learnt to make an informed decision | The main difference betwe\\nen deep and machine learning is, machine learning models become better \\nprogressively but the model still needs some guidance. If a machine-\\nlearning model returns an inaccurate prediction then the programmer need\\ns to fix that problem explicitly but in the case of deep learning, the model do\\nes it by himself.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 9, 'page_label': '10'}, page_content='>Machine Learning can perform well with small size data also | Deep Learn\\ning does not perform as good with smaller datasets. \\n>Machine learning can work on some low-\\nend machines also  | Deep Learning involves many matrix multiplication op\\nerations which are better suited for GPUs \\n>Features need to be identified and extracted as per the domain before pu\\nshing them to the algorithm | Deep learning algorithms try to learn high-\\nlevel features from data. \\n>It is generally recommended to break the problem into smaller chunks, sol\\nve them and then combine the results | It generally focusses on solving the \\nproblem end to end \\n>Training time is comparatively less | Training time is comparatively more \\n>Results are more interpretable | Results Maybe more accurate but less int\\nerpretable \\n> No use of Neural networks | uses neural networks \\n> Solves comparatively less complex problems | Solves more complex prob\\nlems. \\nQ6: What is perceptron and how it is related to human neurons?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 9, 'page_label': '10'}, page_content='If we focus on the structure of a biological neuron, it has dendrites, which \\nare used to receive inputs. These inputs are summed in the cell body and \\nusing the Axon it is passed on to the next biological neuron as shown \\nbelow.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 10, 'page_label': '11'}, page_content='Dendrite: Receives signals from other neurons \\nCell Body: Sums all the inputs \\nAxon: It is used to transmit signals to the other cells \\nSimilarly, a perceptron receives multiple inputs, applies various \\ntransformations and functions and provides an output. A Perceptron is a \\nlinear model used for binary classification. It models a neuron, which has a \\nset of inputs, each of which is given a specific weight. The neuron \\ncomputes some function on these weighted inputs and gives the output.  \\n \\n \\nQ7: Why deep learning is better than machine learning? \\nThough traditional ML algorithms solve a lot of our cases, they are not \\nuseful while working with high dimensional data that is where we have a \\nlarge number of inputs and outputs. For example, in the case of \\nhandwriting recognition, we have a large amount of input where we will \\nhave different types of inputs associated with different types of handwriting.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 11, 'page_label': '12'}, page_content='The second major challenge is to tell the computer what are the features it \\nshould look for that will play an important role in predicting the outcome a s \\nwell as to achieve better accuracy while doing so. \\nQ8: What kind of problem can be solved by using deep learning?  \\nDeep Learning is a branch of Machine Learning, which is used to solve \\nproblems in a way that mimics the human way of solving problems. \\nExamples: \\n\\uf0b7 Image recognition \\n\\uf0b7 Object Detection \\n\\uf0b7 Natural Language processing- Translation, Sentence formations, text \\nto speech, speech to text \\n\\uf0b7 understand the semantics of actions \\nQ9: List down all the activation function using mathematical    \\n       Expression and example. What is the activation function? \\nActivation functions are very important for an Artificial Neural Network to \\nlearn and make sense of something complicated and the Non-linear \\ncomplex functional mappings between the inputs and response variable.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 11, 'page_label': '12'}, page_content='They introduce non-linear properties to our Network. Their main purposes \\nare to convert an input signal of a node in an A-NN to an output signal. \\nSo why do we need Non-Linearities?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 12, 'page_label': '13'}, page_content='Non-linear functions are those, which have a degree more than one, and \\nthey have a curvature when we plot a Non-Linear function. Now we need a \\nNeural Network Model to learn and represent almost anything and any \\narbitrary complex function, which maps inputs to outputs. Neural-Networks \\nare considered Universal Function Approximations. It means that they can \\ncompute and learn any function at all. \\nMost popular types of Activation functions - \\n\\uf0b7 Sigmoid or Logistic \\n\\uf0b7 Tanh — Hyperbolic tangent \\n\\uf0b7 ReLu -Rectified linear units \\nSigmoid Activation function: It is a activation function of form f(x) = 1 / 1 \\n+ exp(-x) . Its Range is between 0 and 1. It is an S-shaped curve. It is easy \\nto understand. \\n \\nHyperbolic Tangent function- Tanh : It’s mathematical formula is f(x) = 1 \\n— exp(-2x) / 1 + exp(-2x). Now it’s the output is zero centred because its \\nrange in between -1 to 1 i.e. -1 < output < 1 . Hence optimisation is easier'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 12, 'page_label': '13'}, page_content='in this method; Hence in practice, it is always preferred over Sigmoid \\nfunction.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 13, 'page_label': '14'}, page_content='ReLu- Rectified Linear units: It has become more popular in the past \\ncouple of years. It was recently proved that it has six times improvement in \\nconvergence from Tanh function. It’s R(x) = max (0,x) i.e. if x < 0 , R(x) = 0 \\nand if x >= 0 , R(x) = x. Hence as seen that mathematical form of this \\nfunction, we can see that it is very simple and efficient. Many times in \\nMachine learning and computer science we notice that most simple and \\nconsistent techniques and methods are only preferred and  are the best. \\nHence, it avoids and rectifies the vanishing gradient problem. Almost all the \\ndeep learning Models use ReLu nowadays. \\n \\n \\nQ10: Detail explanation about gradient decent using example and  \\n       Mathematical expression?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 14, 'page_label': '15'}, page_content='Gradient descent is an optimisation algorithm used to minimize some \\nfunction by iteratively moving in the direction of steepest descent as \\ndefined by negative of the gradient. In machine learning, we used gradient \\ndescent to update the parameters of our model. Parameters refer t o \\ncoefficients in the Linear Regression and weights in neural networks.  \\n \\nThe size of these steps called the learning rate. With the high learning rate, \\nwe can cover more ground each step, but we risk overshooting the lower \\npoint since the slope of the hill is constantly changing. With a very lower \\nlearning rate, we can confidently move in the direction of the negativ e \\ngradient because we are recalculating it so frequently. The Lower learning \\nrate is more precise, but calculating the gradient is time -consuming, so it \\nwill take a very large time to get to the bottom. \\nMath \\nNow let’s run gradient descent using new cost function. There are two'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 14, 'page_label': '15'}, page_content='parameters in cost function we can control: m (weight) and b (bias). Since \\nwe need to consider that the impact each one has on the final prediction, \\nwe need to use partial derivatives. We calculate the partial derivative of the \\ncost function concerning each parameter and store the results in a \\ngradient. \\nMath \\nGiven the cost function:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 15, 'page_label': '16'}, page_content='To solve for the gradient, we iterate by our data points using our new m \\nand b values and compute the partial derivatives. This new gradient te lls us \\nabout the slope of the cost function at our current position (current \\nparameter values) and the directions we should move to update our \\nparameters. The learning rate controls the size of our update. \\nQ11: What is backward propagation?  \\nBack-propagation is the essence of the neural net training and this \\nmethod of fine-tuning the weights of a neural net based on the errors rate \\nobtained in the previous epoch. Proper tuning of the weights allows us to \\nreduce error rates and to make the model reliable by increasing its \\ngeneralisation. \\nBackpropagation is a short form of \"backward propagation of errors.\" This \\nis the standard method of training artificial neural networks. This helps to \\ncalculate the gradient of a loss function with respects to all the weights in \\nthe network.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 16, 'page_label': '17'}, page_content='Most prominent advantages of Backpropagation are: \\n\\uf0b7 Backpropagation is the fast, simple and easy to program. \\n\\uf0b7 It has no parameters to tune apart from the numbers of input . \\n\\uf0b7 It is the flexible method as it does not require prior knowledge about \\nthe network \\n\\uf0b7 It is the standard method that generally works well. \\n\\uf0b7 It does not need any special mentions of the features of the function \\nto be learned. \\n \\n \\n \\nQ12: How we assign weights in deep learning? \\nWe already know that in a neural network, weights are usually initialised \\nrandomly and that kind of initialisation takes a fair/significant amount of \\nrepetitions to converge to the least loss and reach the ideal wei ght matrix. \\nThe problem is, that kind of initialisation is prone to vanishing or exploding \\ngradient problems.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 17, 'page_label': '18'}, page_content='General ways to make it initialise better weights: \\nReLu activation function in the deep nets. \\n1. Generate a random sample of weights from a Gaussian \\ndistribution having mean 0 and a standard deviation of 1.  \\n2. Multiply the sample with the square root of (2/ni). Where ni is the \\nnumber of input units for that layer. \\nb) Likewise, if you’re using Tanh activation function : \\n1. Generate a random sample of weights from a Gaussian distribution \\nhaving mean 0 and a standard deviation of 1. \\n2. Multiply the sample with the square root of (1/ni) where ni is several \\ninput units for that layer. \\n \\nQ13: What is optimiser is deep learning, and which one is the best? \\nDeep learning is an iterative process. With so many hyperparameters to \\ntune or methods to try, it is important to be able to train models fast, to \\nquickly complete the iterative cycle. This is the key to increase the speed \\nand efficiency of a machine learning team.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 17, 'page_label': '18'}, page_content='Hence the importance of optimisation algorithms such as stochastic \\ngradient descent, min-batch gradient descent, gradient descent with \\nmomentum and the Adam optimiser. \\nAdam optimiser is the best one. \\nGiven an algorithm f(x), it helps in either minimisation or maximisation of \\nthe value of f(x). In this context of deep learning, we use optimisation \\nalgorithms to train the neural network by optimising the cost function J.  \\nThe cost function is defined as:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 18, 'page_label': '19'}, page_content='The value of the cost function J is the mean of the loss L between the \\npredicted value y’ and actual value y. The value y” is obtained during the \\nforward propagation step and makes use of the Weights W and biases b of \\nthe network. With the help of optimisation algorithms, we minimise the \\nvalue of Cost Function J  by updating the values of trainable \\nparameters W and b. \\nQ14: What is gradient descent, mini-batch gradient descent, batch  \\n         gradient decent, stochastic gradient decent and adam?  \\nGradient Descent \\nit is an iterative machine learning optimisation algorithm to reduce the cost \\nfunction, and help models to make accurate predictions. \\nGradient indicates the direction of increase. As we want to find the \\nminimum points in the valley, we need to go in the opposite direction of the \\ngradient. We update the parameters in the negative gradient direction to \\nminimise the loss. \\n \\nWhere θ is the weight parameter, η is the learning rate, and ∇J(θ;x,y) is the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 18, 'page_label': '19'}, page_content='gradient of weight parameter θ \\nTypes of Gradient Descent \\nDifferent types of Gradient descents are \\n\\uf0b7 Batch Gradient Descent or Vanilla Gradient Descent \\n\\uf0b7 Stochastic Gradient Descent'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 19, 'page_label': '20'}, page_content='\\uf0b7 Mini batch Gradient Descent \\nBatch Gradient Descent \\nIn the batch gradient, we use the entire dataset to compute the gradient of \\nthe cost function for each iteration for gradient descent and then update the \\nweights. \\nStochastic Gradient descent \\nStochastic gradient descent, we use a single data point or example to \\ncalculate the gradient and update the weights with every iteration.  \\nWe first need to shuffle the datasets so that we get a completely \\nrandomised dataset. As the datasets are random and weights, are updated \\nfor every single example, an update of the weights and the cost functions \\nwill be noisy jumping all over the place  \\nMini Batch Gradient descent \\nMini-batch gradients is a variation of stochastic gradient descent where \\ninstead of a single training example, a mini-batch of samples are used. \\nMini -batch gradient descent is widely used and converges faster and is \\nmore stable. \\nThe batch size can vary depending upon the dataset.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 19, 'page_label': '20'}, page_content='As we take batches with different samples, it reduces the noise which is a \\nvariance of the weights updates, and that helps to have a more stable \\nconverge faster. \\nQ15: What are autoencoders? \\nAn autoencoder, neural networks that have three layers: \\nAn input layer, a hidden layer which is also known as encoding layer, and a \\ndecoding layer. This network is trained to reconstruct its inputs, which \\nforces the hidden layer to try to learn good representations of the inputs.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 20, 'page_label': '21'}, page_content='An autoencoder neural network is an unsupervised Machine-learning \\nalgorithm that applies backpropagation, setting the target values to be \\nequal to the inputs. An autoencoder is trained to attempts to copy its input \\nto its output. Internally, it has a hidden layer which describes a code used \\nto represent the input. \\n \\nAutoencoder Components: \\nAutoencoders consists of 4 main parts: \\n1- Encoder: In this, the model learns how to reduce the input dimensions \\nand compress the input data into an encoded representation.  \\n2- Bottleneck: In this, the layer that contains the compressed \\nrepresentation of the input data. This is the lowest possible dimension of \\nthe input data. \\n3- Decoder: In this, the model learns how to reconstruct the data from the \\nencod represented to be as close to the original inputs as possible. \\n4- Reconstruction Loss: In this method that measures measure how well \\nthe decoder is performing and how closed the output is related to  the \\noriginal input.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 20, 'page_label': '21'}, page_content='original input. \\nTypes of Autoencoders : \\n1. Denoising auto encoder \\n2. Sparse auto encoder \\n3. Variational auto encoder (VAE) \\n4. Contractive auto encoder (CAE)'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 21, 'page_label': '22'}, page_content='Q16: What  is CNN? \\nThis is the simple application of a filter to an input that results  in \\ninactivation. Repeated application of the same filter to input results in a \\nmap of activations called a feature map, indicating the locations and \\nstrength of a detected feature in input, such as an image.  \\nConvolutional layers are the major building blocks which are used in \\nconvolutional neural networks. \\nA covnets is the sequence of layers, and every layer transforms one \\nvolume to another through differentiable functions. \\nDifferent types of layers in CNN: \\nLet’s take an example by running a covnets on of image of dimension s 32 x \\n32 x 3. \\n1. Input Layer: It holds the raw input of image with width 32, height 32 \\nand depth 3. \\n2. Convolution Layer: It computes the output volume by computing dot \\nproducts between all filters and image patches. Suppose we use a \\ntotal of 12 filters for this layer we’ll get output volume of dimension 32 \\nx 32 x 12.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 21, 'page_label': '22'}, page_content='x 32 x 12. \\n3. Activation Function Layer: This layer will apply the element-wise \\nactivation function to the output of the convolution layer. Some  \\nactivation functions are RELU: max(0, x), Sigmoid: 1/(1+e ^-x), Tanh, \\nLeaky RELU, etc. So the volume remains unchanged. Hence output \\nvolume will have dimensions 32 x 32 x 12. \\n4. Pool Layer: This layer is periodically inserted within the covnets, and \\nits main function is to reduce the size of volume which makes the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 22, 'page_label': '23'}, page_content='computation fast reduces memory and also prevents overfitting. Two \\ncommon types of pooling layers are max pooling and average \\npooling. If we use a max pool with 2 x 2 filters and stride 2, the \\nresultant volume will be of dimension 16x16x12. \\n \\n5. Fully-Connected Layer: This layer is a regular neural network layer \\nthat takes input from the previous layer and computes the class \\nscores and outputs the 1-D array of size equal to the number of \\nclasses. \\n \\n \\nQ17: What is pooling, padding, filtering operations on CNN? \\nPooling Layer \\nIt is commonly used to periodically insert a Pooling layer in-between \\nsuccessive Conv layers in a ConvNet architecture. Its function is to \\nprogressively reduce the spatial size of the representation to reduce the \\nnumber of parameters and computation in the network, and hence to also'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 23, 'page_label': '24'}, page_content='control overfitting. The Pooling Layer operates independently on every \\ndepth slice of the input and resizes it spatially, using the MAX operation.  \\n \\nThe most common form is a pooling layer with filters of size 2x2 applied \\nwith a stride of 2 downsamples every depth slice in the input by two along \\nboth width and height, discarding 75% of the activations. Every MAX \\noperation would, in this case, be taking a max over four numbers (little 2x2 \\nregion in some depth slice). The depth dimension remains unchanged.  \\n \\n \\n \\nQ18: What is the Evolution technique of CNN?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 24, 'page_label': '25'}, page_content='It all started with LeNet in 1998 and eventually, after nearly 15 years, lead \\nto groundbreaking models winning the ImageNet Large Scale Visual \\nRecognition Challenge which includes AlexNet in 2012 to Google Net in \\n2014 to ResNet in 2015 to an ensemble of previous models in 2016. In the \\nlast two years, no significant progress has been made, and the new models \\nare an ensemble of previous groundbreaking models. \\n \\nLeNet in 1998 \\nLeNet is a 7-level convolutional network by LeCun in 1998 that classifies \\ndigits and used by several banks to recognise the hand-written numbers on \\ncheques digitised in 32x32 pixel greyscale input images.  \\nAlexNet in 2012 \\nAlexNet: It is considered to be the first paper/ model, which rose the \\ninterest in CNNs when it won the ImageNet challenge in the year 2012. It is \\na deep CNN trained on ImageNet and outperformed all the entries that \\nyear. \\nVGG in 2014 \\nVGG was submitted in the year 2013, and it became a runner up in the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 24, 'page_label': '25'}, page_content='ImageNet contest in 2014. It is widely used as a simple architecture \\ncompared to AlexNet.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 25, 'page_label': '26'}, page_content=\"GoogleNet in 2014 \\nIn 2014, several great models were developed like VGG, but the winner of \\nthe ImageNet contest was GoogleNet. \\nGoogLeNet proposed a module called the inception modules that includes \\nskipping connections in the network, forming a mini-module, and this \\nmodule is repeated throughout the network. \\nResNet in 2015 \\nThere are 152 layers in the Microsoft ResNet. The authors showed \\nempirically that if you keep on adding layers, the error rate should keep on \\ndecreasing in contrast to “plain nets” we're adding a few layers resulted in \\nhigher training and test errors. \\nQ19: How to initialise biases in deep learning? \\nIt is possible and common to initialise the biases to be zero since the \\nrandom numbers in the weights provide the asymmetry braking . For ReLU \\nnon-linearities, some people like to use small constant value such as 0.01 \\nfor all biases because this ensures that all ReLU units fire in the beginning,\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 25, 'page_label': '26'}, page_content='therefore obtain, and propagate some gradient. However, it is unclear if this \\nprovides a consistent improvement (in fact some results seem to indicate s \\nthat this performs worst) and it is more commonly used to use 0 bias \\ninitialisation. \\nQ20: What is learning Rate? \\nLearning Rate \\nThe learning rate controls how much we should adjust the weights \\nconcerning the loss gradient. Learning rates are randomly initialised. \\nLower the values of the learning rate slower will be the convergence to \\nglobal minima. \\nHigher values for the learning rate will not allow the gradient descent to \\nconverge'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2019-11-03T16:04:29+00:00', 'author': 'Sourangshu Pal', 'moddate': '2019-11-03T16:04:29+00:00', 'source': 'Data Science Interview Preparation(#DAY 04).pdf', 'total_pages': 27, 'page': 26, 'page_label': '27'}, page_content='Since our goal is to minimise the  function cost to find the optimised value \\nfor weights, we run multiples iteration with different weights and calculate \\nthe cost to arrive at a minimum cost  \\n \\n----------------------------------------------------------------------------------------------------\\n------------------------')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e2cacb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaibhav Singh\\AppData\\Local\\Temp\\ipykernel_9560\\2100376373.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  db=FAISS.from_documents(documents,OllamaEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db=FAISS.from_documents(documents,OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d54b606e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x24bd604a710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8090c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>Machine Learning can perform well with small size data also | Deep Learn\\ning does not perform as good with smaller datasets. \\n>Machine learning can work on some low-\\nend machines also  | Deep Learning involves many matrix multiplication op\\nerations which are better suited for GPUs \\n>Features need to be identified and extracted as per the domain before pu\\nshing them to the algorithm | Deep learning algorithms try to learn high-\\nlevel features from data. \\n>It is generally recommended to break the problem into smaller chunks, sol\\nve them and then combine the results | It generally focusses on solving the \\nproblem end to end \\n>Training time is comparatively less | Training time is comparatively more \\n>Results are more interpretable | Results Maybe more accurate but less int\\nerpretable \\n> No use of Neural networks | uses neural networks \\n> Solves comparatively less complex problems | Solves more complex prob\\nlems. \\nQ6: What is perceptron and how it is related to human neurons?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\" Why deep learning is better than machine learning?\"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95194932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaibhav Singh\\AppData\\Local\\Temp\\ipykernel_9560\\2277366689.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model=\"llama2\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "## Load Ollama LAMA2 LLM model\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7ec2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Introduction\n",
    "## Create Stuff Docment Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f40aa963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024BD604A710>, search_kwargs={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e47e4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3364ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers are a type of neural network architecture that have gained popularity in recent years, particularly in the field of natural language processing (NLP). They were introduced in a 2017 paper by Vaswani et al. titled \"Attention is All You Need,\" and have since become a standard component in many NLP models.\\n\\nIn essence, transformers are a type of encoder-decoder architecture that uses self-attention mechanisms to process input sequences. Unlike traditional recurrent neural networks (RNNs), which process sequences one element at a time and require recurrence to capture long-range dependencies, transformers parallelize the computation of self-attention over all elements in a sequence, allowing them to handle much longer input sequences efficiently.\\n\\nThe key innovation of transformers is the use of self-attention mechanisms, which allow the model to attend to different parts of the input sequence simultaneously and weigh their importance when computing the output. This allows transformers to capture complex contextual relationships in input sequences more effectively than traditional RNNs.\\n\\nIn NLP, transformers have been used for a wide range of tasks, including language translation, text generation, and question answering. They have achieved state-of-the-art results in many of these tasks, often outperforming traditional RNNs and other neural network architectures.\\n\\nSome of the key benefits of transformers include:\\n\\n1. Parallelization: Transformers can parallelize the computation of self-attention over all elements in a sequence, allowing them to handle much longer input sequences efficiently than traditional RNNs.\\n2. Flexibility: Transformers are highly flexible and can be easily adapted to different NLP tasks by changing the input and output layers.\\n3. Contextual understanding: Transformers use self-attention mechanisms to capture complex contextual relationships in input sequences, allowing them to understand the meaning of input text more effectively than traditional RNNs.\\n4. Efficient use of parameters: Transformers make efficient use of parameters by learning multiple representations of the input sequence simultaneously, rather than relying on a single representation as in traditional RNNs.\\n\\nOverall, transformers represent a significant advancement in NLP neural network architectures and have enabled the development of more sophisticated and accurate NLP models.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"What is Transformers in NLP\"})\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261fc9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
